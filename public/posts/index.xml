<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Zhanxiong&#39;s Blog</title>
    <link>http://localhost:1313/ZX_BLOG/posts/</link>
    <description>Recent content in Posts on Zhanxiong&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 10 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/ZX_BLOG/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rendering Images in GitHub Pages</title>
      <link>http://localhost:1313/ZX_BLOG/posts/2025-06-10-hugoimages/</link>
      <pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ZX_BLOG/posts/2025-06-10-hugoimages/</guid>
      <description>&lt;p&gt;When I built my &lt;a href=&#34;https://littlecottage.github.io/ZX_BLOG/&#34;&gt;personal blog site&lt;/a&gt; with &lt;a href=&#34;https://themes.gohugo.io/themes/hugo-texify3/&#34;&gt;Hugo Texify3&lt;/a&gt; and tried deploying it &lt;a href=&#34;https://gohugo.io/host-and-deploy/host-on-github-pages/&#34;&gt;using GitHub Pages&lt;/a&gt;, something very weird happened: for &lt;a href=&#34;https://littlecottage.github.io/ZX_BLOG/posts/2025-01-26-ce-evaluation/&#34;&gt;a blog&lt;/a&gt; containing images, while those images can be shown locally under the URL &lt;code&gt;http://localhost:1313&lt;/code&gt;, they cannot be rendered under GitHub Pages.&lt;/p&gt;
&lt;p&gt;After muddling this issue with GitHub Copilot embedded in VS Code for hours, eventually ChatGPT saved my day. The reason is actually simple: in the post, the image was initially referenced as&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;![case 1 visualization](/images/2025/case_1.png)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This works fine locally (it resolves to &lt;code&gt;./static/images/2025/case_1.png&lt;/code&gt;) but breaks remotely (it resolves to &lt;code&gt;https://littlecottage.github.io/public/images/2025/case_1/png&lt;/code&gt; while the correct location is &lt;code&gt;https://littlecottage.github.io/ZX_BLOG/public/images/2025/case_1/png&lt;/code&gt;).  So a quick fix is updating the reference to&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;![case 1 visualization](/ZX_BLOG/images/2025/case_1.png)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;so that it resolves to the correct path both locally and remotely. To ensure that the path was searched in an absolute way rather than relatively, then adding the statement&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;canonifyURLs = true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;to the &lt;code&gt;hugo.toml&lt;/code&gt; configuration file so that Hugo converts all relative URLs to absolute URLs using &lt;code&gt;baseURL = https://littlecottage.github.io/ZX_BLOG/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The learned lesson is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;When an object failed to display in the browser, 99% is a path specification issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trust ChatGPT more than Copilot.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A complete chat history with ChatGPT &lt;a href=&#34;https://chatgpt.com/share/68488b35-6a9c-8012-8bf3-0013a4bc95f2&#34;&gt;is here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Kernel Properties</title>
      <link>http://localhost:1313/ZX_BLOG/posts/2025-06-09-gaussian/</link>
      <pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ZX_BLOG/posts/2025-06-09-gaussian/</guid>
      <description>&lt;p&gt;Your question can be reframed as: why the statement (a) $X_t \mid X_{t - 1} \sim N(\sqrt{1 - \beta_t}X_{t - 1}, \beta_t I)$ is equivalent to the statement (b) $X_t$ has the representation $X_t = \sqrt{1 - \beta_t}X_{t - 1} + \sqrt{\beta_t}Z_t$, where $Z_t \sim N(0, I)$ is independent of $X_{t - 1}$?&lt;/p&gt;
&lt;p&gt;While Thomas Lumley has given you a heuristic answer, I agree that you are absolutely entitled to request a more rigorous argument.&lt;/p&gt;
&lt;p&gt;The direction $(b) \Rightarrow (a)$ is an immediate application of the theorem below (Lemma 8.7 in Foundations of Modern Probability (3rd edition) by Olav Kallenberg):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For any random elements $\xi \perp \eta$ in $S, T$ and a measurable function $f: S \times T \to U$, where $S, T, U$ are Borel, define $X = f(\xi, \eta)$. Then $\mathcal{L}(X \mid \xi) = \mathcal{L}\{f(x, \eta)\}|_{x = \xi}$ a.s.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Here &amp;ldquo;$\mathcal{L}(X)$ / $\mathcal{L}(X \mid \xi)$&amp;rdquo; denotes the law (i.e., distribution) of $X$ / conditional law of $X$ given $\xi$. To apply this result, simply let $\xi = X_{t - 1}, \eta = Z_t$, and $X = X_t = \sqrt{1 - \beta_t}X_{t - 1} + \sqrt{\beta_t}Z_t$. If you are interested in the proof to this lemma, check the reference by yourself.&lt;/p&gt;
&lt;p&gt;For the direction $(a) \Rightarrow (b)$, it suffices to prove the conditional distribution of $Z_t := \beta_t^{-1/2}(X_t - \sqrt{1 - \beta_t}X_{t - 1})$ given $X_{t - 1}$ is $N(0, I)$, hence it is independent of $X_{t - 1}$. To this end, we can apply Theorem 8.5(ii) in the same reference (which concerns with the disintegration of the joint distribution of $(X_{t - 1}, X_t)$).  By this theorem, for a Borel set $H \in \mathscr{R}^n$, we have (where the notation $aH + b$ denotes the set ${ax + b: x \in H}$):
$$
\begin{align*}
&amp;amp;\ P(Z_t \in H \mid X_{t - 1}) \\
=&amp;amp;\ P\left(X_t \in \sqrt{\beta_t}H + \sqrt{1 - \beta_t}X_{t - 1} \mid X_{t - 1}\right) \\
=&amp;amp;\ \int_{\sqrt{\beta_t}H + \sqrt{1 - \beta_t}X_{t - 1}}
(2\pi\beta_t)^{-n/2}\exp\left(-\frac{\beta_t^{-n}}{2}(x - \sqrt{1 - \beta_t}X_{t - 1})^\top(x - \sqrt{1 - \beta_t}X_{t - 1})\right)dx \\
=&amp;amp;\ \int_H (2\pi)^{-n/2}\exp\left(-\frac{1}{2}z^\top z\right)dz.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;where the last step uses the transformation $z = \beta_t^{-n/2}(x - \sqrt{1 - \beta_t}X_{t - 1})$ and the property of the Lebesgue integration. The last expression shows that the conditional distribution of $Z_t$ given $X_{t - 1}$ is exactly $N(0, I)$.  This completes the proof.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geometric Intuition for a Variance Identity</title>
      <link>http://localhost:1313/ZX_BLOG/posts/2025-03-17-variance/</link>
      <pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ZX_BLOG/posts/2025-03-17-variance/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://stats.stackexchange.com/questions/662731/geometric-intuition-for-the-identity-operatornamevarx-y-operatorname&#34;&gt;question&lt;/a&gt; is: what is geometric intuition for the identity $\operatorname{Var}(X + Y) = \operatorname{Var}(X - Y)$ given that $X$ and $Y$ are independent?&lt;/p&gt;
&lt;p&gt;To interpret this identity geometrically, it is helpful to view $\operatorname{Var}(X + Y)$ and $\operatorname{Var}(X - Y)$ as norms in the Hilbert space $L^2(\Omega, \mathscr{F}, P)$ equipped with inner product $\langle X, Y \rangle = E[XY]$.&lt;/p&gt;
&lt;p&gt;To this end, write $\operatorname{Var}(X + Y)$ and $\operatorname{Var}(X - Y)$ as
$$
\begin{align*}
&amp;amp; \operatorname{Var}(X + Y) =
E{[(X - \mu_X) + (Y - \mu_Y)]^2} = \|(X - \mu_X) + (Y - \mu_Y)\|^2, \\
&amp;amp; \operatorname{Var}(X - Y) =
E{[(X - \mu_X) - (Y - \mu_Y)]^2} = \|(X - \mu_X) - (Y - \mu_Y)\|^2.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Therefore, by viewing $X - \mu_X$ and $Y - \mu_Y$ as two vectors in $L^2(\Omega, \mathscr{F}, P)$, $\operatorname{Var}(X + Y)$ and $\operatorname{Var}(X - Y)$ are squared lengths of the blue diagonal and the red diagonal in the parallelogram below respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/var_general.png&#34; alt=&#34;variance general case&#34;&gt;&lt;/p&gt;
&lt;p&gt;When $X$ and $Y$ are independent, $X - \mu_X$ and $Y - \mu_Y$ are clearly independent too, which means that $X - \mu_X$ and $Y - \mu_Y$ are orthogonal. That is, the parallelogram in the above diagram becomes a rectangle as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/var_independent.png&#34; alt=&#34;variance independent case&#34;&gt;&lt;/p&gt;
&lt;p&gt;As diagonals in a rectangle are of the same length, we obtain $\operatorname{Var}(X + Y) = \operatorname{Var}(X - Y)$ geometrically.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Interesting Conditional Expectation Problem</title>
      <link>http://localhost:1313/ZX_BLOG/posts/2025-01-26-ce-evaluation/</link>
      <pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ZX_BLOG/posts/2025-01-26-ce-evaluation/</guid>
      <description>&lt;p&gt;The problem is posted on &lt;a href=&#34;https://stats.stackexchange.com/questions/660531/conditional-expectation-when-there-are-no-symmetries&#34;&gt;CrossValidated&lt;/a&gt; by a Spanish professor. It is stated as:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Suppose $(X, Y, Z) \sim f(x, y, z) = \frac{1}{3}(x + 2y + 3z), 0 &amp;lt; x, y, z &amp;lt; 1$ and $S = X + Y + Z$. Prove that $E[Y \mid S] = \frac{S}{3}$.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;when-the-form-of-ey-mid-s-is-given&#34;&gt;When the form of $E[Y \mid S]$ is given&lt;/h2&gt;
&lt;p&gt;Provided the answer is $E[Y \mid S] = \frac{S}{3}$, it suffices to show that
$$
\begin{align*}
\int_{S \leq s} Y dP = \int_{S \leq s} \frac{1}{3}SdP,
\end{align*}
$$
which, by the change-of-variable formula, is
$$
\begin{align*}
\iiint_V g(x, y, z)dxdydz = 0, \tag{1}
\end{align*}
$$
where
$$
\begin{align*}
&amp;amp; g(x, y, z) = (2y - x - z)(x + 2y + 3z), \\
&amp;amp; V = \{(x, y, z): 0 &amp;lt; x, y, z &amp;lt; 1, x + y + z \leq s\}.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;Depending on whether $s \in (0, 1]$ or $s \in (1, 2)$ or $s \in [2, 3)$, $V$ has different forms, resulting different difficulty levels of the integral evaluation.&lt;/p&gt;
&lt;p&gt;After converting the triple integral in $(1)$ to an iterative integral, we could resort to software to finish the mundane evaluation task (e.g., using Mathematica or Python, here I used the &lt;code&gt;sympy&lt;/code&gt; package in Python to help me finish the work).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 1.&lt;/strong&gt; $s \in (0, 1]$. In this case, the projection of $V$ onto the $x$-$y$ plane is
$$
\begin{align*}
\Delta = \{(x, y): 0 \leq y \leq s - x, 0 \leq x \leq s\}.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The visualization of $V$ and $\Delta$ is shown in the figure below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/case_1.png&#34; alt=&#34;case 1 visualization&#34;&gt;&lt;/p&gt;
&lt;!-- 
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/case_1.png&#34; alt=&#34;case 1 visualization&#34; style=&#34;display: block; margin: 0 auto;&#34; width=&#34;500&#34;/&gt;
&lt;/p&gt;
--&gt;
&lt;p&gt;Therefore, $(1)$ is equivalent to
$$
\begin{align*}
&amp;amp; \iiint_Vg(x, y, z)dxdydz = \iint_\Delta dydx\int_0^{s - x - y}g(x, y, z)dz = \int_0^s\int_0^{s - x}\int_0^{s - x - y}g(x, y, z)dzdydx = 0.  \tag{2}
\end{align*}
$$
$(2)$ indeed holds (see &lt;code&gt;Case 1&lt;/code&gt; in the code chunk below).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 2.&lt;/strong&gt; $s \in (1, 2)$. In this case, the projection of $V$ onto the $x$-$y$ plane is a pentagon, which is a disjoint union of a triangle $\Delta = \{(x, y): x + y \leq  s - 1, x, y \geq 0\}$ and two trapezoids $T_1, T_2$, where
$$
\begin{align*}
&amp;amp; T_1 = \{(x, y):  s - 1 - x \leq y \leq 1, 0 \leq x \leq s - 1\}, \\
&amp;amp; T_2 = \{(x, y):  0 \leq y \leq s - x, s - 1 \leq x \leq 1\}.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The visualization of $V$, $\Delta$, $T_1$ and $T_2$ is shown in the figure below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/case_2.png&#34; alt=&#34;case 2 visualization&#34;&gt;&lt;/p&gt;
&lt;!-- 
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/case_2.png&#34; alt=&#34;case 2 visualization&#34; style=&#34;display: block; margin: 0 auto;&#34; width=&#34;500&#34;/&gt;
&lt;/p&gt;
--&gt;
&lt;p&gt;From the 3D plot of $V$, it is clear that on $\Delta$, the integration interval for $z$ is $[0, 1]$, and on $T_1 \cup T_2$, the integration interval for $z$ is $[0, s - x - y]$. Therefore, $(1)$ is equivalent to
$$
\begin{align*}
&amp;amp; \iint_{\Delta}dydx\int_0^1g(x, y, z)dz +
\iint_{T_1 \cup T_2}dydx\int_0^{s - x - y}g(x, y, z)dz \\
=&amp;amp; \int_0^{s - 1}\int_0^{s - 1 - x}\int_0^1g(x, y, z)dzdx + \int_0^{s - 1}\int_{s - 1 - x}^1\int_0^{s - x - y}g(x, y, z)dzdydx + \int_{s - 1}^1\int_0^{s - x}\int_0^{s - x - y}g(x, y, z)dzdydx \\
=&amp;amp; 0.\tag{3}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;$(3)$ indeed holds (see &lt;code&gt;Case 2&lt;/code&gt; in the code chunk below).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Case 3.&lt;/strong&gt; $s \in [2, 3)$. In this case, the projection of $V$ onto the $x$-$y$ plane is the square $[0, 1] \times [0, 1]$, which is a disjoint union of a rectangle $R$, a trapezoid $T$ and a triangle $\Delta$, where
$$
\begin{align*}
&amp;amp; R = \{(x, y): 0 \leq y \leq 1, 0 \leq x \leq s - 2\}, \\
&amp;amp; T = \{(x, y): 0 \leq y \leq s - 1 - x, s - 2 \leq x \leq 1\}, \\
&amp;amp; \Delta = \{(x, y): s - 1 - x \leq y \leq 1, s - 2 \leq x \leq 1\}.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The visualization of $V$, $R$, $T$ and $\Delta$ is shown in the figure below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/case_3.png&#34; alt=&#34;case 3 visualization&#34;&gt;&lt;/p&gt;
&lt;!-- 
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/case_3.png&#34; alt=&#34;case 3 visualization&#34; style=&#34;display: block; margin: 0 auto;&#34; width=&#34;500&#34;/&gt;
&lt;/p&gt;
--&gt;
&lt;p&gt;From the 3D plot of $V$, it is clear that on $R \cup T$, the integration interval for $z$ is $[0, 1]$, and on
$\Delta$, the integration interval for $z$ is $[0, s - x - y]$. Therefore, $(1)$ is equivalent to prove
$$
\begin{align*}
&amp;amp; \iiint_Vg(x, y, z)dxdydz \\
=&amp;amp; \iint_{R \cup T}dydx\int_0^1g(x, y, z)dz + \iint_\Delta dydx\int_0^{s - x - y}g(x, y, z)dz \\
=&amp;amp; \int_0^{s - 2}\int_0^1\int_0^1g(x, y, z)dzdydx + \int_{s - 2}^1\int_0^{s - 1 - x}\int_0^1g(x, y, z)dzdydx + \int_{s - 2}^1\int_{s - 1 - x}^1\int_0^{s - x - y}g(x, y, z)dzdydx \\
=&amp;amp; 0. \tag{4}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;$(4)$ indeed holds (see &lt;code&gt;Case 3&lt;/code&gt; in the code chunk below).&lt;/p&gt;
&lt;p&gt;This completes the proof.&lt;/p&gt;
&lt;h2 id=&#34;when-the-form-of-ey-mid-s-is-unknown&#34;&gt;When the form of $E[Y \mid S]$ is unknown&lt;/h2&gt;
&lt;p&gt;If we are not told $E[Y \mid S] = \frac{S}{3}$, (in general) we can first determine $P(Y &amp;gt; y \mid S = s)$ using the approach as in &lt;a href=&#34;https://stats.stackexchange.com/a/648902/20519&#34;&gt;this answer&lt;/a&gt;. Clearly we are still faced with several complicated triple integration problems and the analysis/tool in the proof above can be applied in a similar (but probably more verbose) manner. For this particular problem, because $(Y, S)$ has a non-degenerate density, this approach could be reduced to a more elementary method &amp;ndash; that is, computing the conditional density $f(y \mid s)$ of $Y$ given $S$ directly using the formula
$$
\begin{align*}
f(y \mid s) = \frac{f(y, s)}{\int_0^s f(v, s)dv}, \quad 0 &amp;lt; y &amp;lt; s.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;The joint density $f(y, s)$ can be determined by following a classical recipe in elementary probability, that is, by first evaluating the probability $F(y, s) := P(Y \leq y, S \leq s)$ then taking second-order partial derivative with respect to $s$ and $y$. As we have seen in the earlier proof, depending on where $s$ is located, the difficulty of computing $F(y, s)$ varies. Nevertheless, the key idea of converting a triple integration to an iterative integration by carefully gauging integration limits for $(x, y, z)$ stays the same. Below I illustrate this process for the (simplest) case $s \in (0, 1]$.&lt;/p&gt;
&lt;p&gt;In this case, for $0 &amp;lt; y &amp;lt; s$, $F(y, s)$ is a triple integration of $f(u, v, w) = \frac{1}{3}(u + 2v + 3w)$ on the pentahedron $V = \{(u, v, w): u + v + w \leq s, 0 \leq u \leq 1, 0 \leq v \leq y\}$. Note that the projection of $V$ onto the $u$-$v$ plane is a trapezoid $T =
\{(u, v): 0 \leq u \leq s - v, 0 \leq v \leq y\}$. See the figure below for the visualization of $V$ and $T$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/direct_eval.png&#34; alt=&#34;direct_eval&#34;&gt;&lt;/p&gt;
&lt;!-- 
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;http://localhost:1313/ZX_BLOG/images/2025/direct_eval.png&#34; alt=&#34;direct eval&#34; style=&#34;display: block; margin: 0 auto;&#34; width=&#34;500&#34;/&gt;
&lt;/p&gt;
--&gt;
&lt;p&gt;Therefore,
$$
\begin{align*}
&amp;amp; F(y, s) = \iiint_V f(u, v, w)dudvdw \\
=&amp;amp; \iint_Tdudv\int_0^{s - u - v}f(u, v, w)dw \\
=&amp;amp; \frac{1}{3}\int_0^y\int_0^{s - v}\int_0^{s - u - v}(u + 2v + 3w)dwdudv \\
=&amp;amp; \frac{2}{9}ys^3 - \frac{1}{6}s^2y^2 + \frac{1}{36}y^4.
\end{align*}
$$
The last integral is again obtained with the help of &lt;code&gt;sympy&lt;/code&gt;. It then follows that
$$
\begin{align*}
f(y, s) = \frac{\partial^2 F(y, s)}{\partial y\partial s} = \frac{2}{3}s(s - y), \quad 0 &amp;lt; y &amp;lt; s.
\end{align*}
$$
whence
$$
\begin{align*}
f(y \mid s) = \frac{f(y, s)}{\int_0^s f(v, s)dv} = \frac{2}{s}\left(1 - \frac{y}{s}\right), \quad 0 &amp;lt; y &amp;lt; s.
\end{align*}
$$
Hence
$$
\begin{align*}
E[Y \mid S = s] = \int_0^syf(y \mid s)dy = \frac{2}{s}\int_0^s\left(y - \frac{y^2}{s}\right)dy = \frac{s}{3},
\end{align*}
$$
as desired.&lt;/p&gt;
&lt;h2 id=&#34;python-code&#34;&gt;Python Code&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; sympy &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; symbols, integrate, simplify
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;x, y, z, s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; symbols(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x y z s&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; z) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; z)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;int_3D&lt;/span&gt;(il, iu, ml, mu, ol, ou):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f_inner &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; integrate(f, (z, il, iu)) 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f_middle &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; integrate(f_inner, (y, ml, mu))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    f_outer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; integrate(f_middle, (x, ol, ou))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; simplify(f_outer)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The result of the triple integral is: &amp;#34;&lt;/span&gt;, res)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; res
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Case 1: 0 &amp;lt; s &amp;lt;= 1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res_c1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int_3D(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# res_c1 = 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Case 2: 1 &amp;lt; s &amp;lt; 2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int_3D(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# I1 = s**4/4 - s**3 + s**2 - 1/4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int_3D(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# I2 = -s**4/2 + 11*s**3/6 - 11*s**2/6 + 5*s/12 + 1/12&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int_3D(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# I3 = s**4/4 - 5*s**3/6 + 5*s**2/6 - 5*s/12 + 1/6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res_c2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; simplify(I1 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; I2 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; I3)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# res_c2 = 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Case 3: 2 &amp;lt;= s &amp;lt; 3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int_3D(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# I1 = -s**3/3 + s**2 + 4*s/3 - 4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int_3D(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# I2 = -s**4/4 + 7*s**3/3 - 13*s**2/2 + 14*s/3 + 7/4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;I3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int_3D(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; y, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; x, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, s &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# I3 = s**4/4 - 2*s**3 + 11*s**2/2 - 6*s + 9/4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;res_c3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; simplify(I1 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; I2 &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; I3)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# res_c3 = 0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>An Alternative Representation of $R^2$</title>
      <link>http://localhost:1313/ZX_BLOG/posts/2023-11-14-r-squared/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ZX_BLOG/posts/2023-11-14-r-squared/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;This question was originally posted on &lt;a href=&#34;https://stats.stackexchange.com/questions/631291/are-these-two-definitions-of-the-coefficient-of-determination-r2-equal&#34;&gt;CrossValidated&lt;/a&gt;.  In more compact notations, it asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a linear model
$$
\begin{align*}
Y := \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
= \begin{bmatrix} e &amp;amp; x_1 &amp;amp; x_2 &amp;amp; \cdots &amp;amp; x_p \end{bmatrix}\beta + \varepsilon,
\end{align*}
$$
show that the well-known &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34;&gt;coefficient of determination&lt;/a&gt; has the following representation:
$$
\begin{align*}
R^2 = r_{YX}^\top r_{XX}^{-1}r_{YX},
\end{align*}
$$
where
$$
\begin{align*}
r_{YX} := \begin{bmatrix} r_{Y, x_1} \\ \vdots \\ r_{Y, x_p} \end{bmatrix}, \quad
r_{XX} := \begin{bmatrix} r_{x_i, x_j} \end{bmatrix}.
\end{align*}
$$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In other words, $R^2$ is a function of pair-wise Pearson correlations, which is surprisingly elegant.&lt;/p&gt;
&lt;h2 id=&#34;the-proof&#34;&gt;The Proof&lt;/h2&gt;
&lt;p&gt;To begin with, rewrite the linear model as
$$
\begin{align*}
Y := \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}
= \begin{bmatrix} e &amp;amp; x_1 &amp;amp; x_2 &amp;amp; \cdots &amp;amp; x_p \end{bmatrix}\beta + \varepsilon := \begin{bmatrix} e &amp;amp; \tilde{X}\end{bmatrix}\begin{bmatrix} \beta_0 \\ \gamma \end{bmatrix} + \varepsilon, \tag{1}
\end{align*}
$$
where $x_j = \begin{bmatrix} x_{1j} &amp;amp; \cdots &amp;amp; x_{nj}\end{bmatrix}^\top$, $j = 1, \ldots, p$, $\beta_0 \in \mathbb{R}$ is the coefficient corresponding to $e$, $\gamma \in \mathbb{R}^p$  is the coefficient corresponding to $\tilde{X} = \begin{bmatrix} x_1 &amp;amp; \cdots &amp;amp; x_p\end{bmatrix} \in \mathbb{R}^{n \times p}$.&lt;/p&gt;
&lt;p&gt;The proposed form of $R^2$ inspires me considering the following representation of a regression model (known as &lt;em&gt;Standardized Multiple Regression Model&lt;/em&gt;, see &lt;em&gt;Applied Linear Statistical Models&lt;/em&gt; by Kutner et al., Section 7.5) &amp;ndash; this is how I managed to bring all the Pearson correlations to the party:
$$
\begin{align*}
Y^* = X^*\beta^* + \varepsilon^*, \tag{2}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;where
$$
\begin{align*}
Y^* = \begin{bmatrix} \frac{y_1 - \bar{y}}{\sqrt{n - 1}s_Y} \\ \vdots \\ \frac{y_n - \bar{y}}{\sqrt{n - 1}s_Y} \end{bmatrix} = (\sqrt{n - 1}s_Y)^{-1}(I - n^{-1}ee^\top)Y \in \mathbb{R}^{n \times 1}, \tag{3.1}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
X^* = \begin{bmatrix} \frac{x_1 - \bar{x}_1e}{\sqrt{n - 1}s_1} &amp;amp; \cdots &amp;amp; \frac{x_p - \bar{x}_pe}{\sqrt{n - 1}s_p} \end{bmatrix} = (I - n^{-1}ee^\top)\tilde{X}(\sqrt{n - 1}\Lambda)^{-1} \in \mathbb{R}^{n \times p}, \tag{3.2}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\bar{y} = \frac{1}{n}\sum_{i = 1}^ny_i, \quad s_Y^2 = \frac{1}{n - 1}\sum_{i = 1}^n(y_i - \bar{y})^2, \tag{3.3}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\bar{x_j} = n^{-1}\sum_{i = 1}^n x_{ij}, \quad s_j^2 = \frac{1}{n - 1}\sum_{i = 1}^n(x_{ij} - \bar{x_j})^2, j = 1, \ldots, p, \tag{3.4}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\Lambda = \operatorname{diag}(s_1, \ldots, s_p) \in \mathbb{R}^{p \times p}. \tag{3.5}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;In words, $Y^*$ and $X^*$ are simply standardized versions of original inputs $Y$ and $\tilde{X}$.&lt;/p&gt;
&lt;p&gt;With the above notations, on one hand, it is easy to verify that
$$
\begin{align*}
r_{YX} := \begin{bmatrix} r_{Y, x_1} \\ \vdots \\ r_{Y, x_p} \end{bmatrix} = X^{*\top}Y^*, \quad
r_{XX} := \begin{bmatrix} r_{x_i, x_j} \end{bmatrix} = X^{*\top}X^*, \tag{4}
\end{align*}
$$
hence
$$
\begin{align*}
\hat{\beta^*} = (X^{*\top}X^*)^{-1}X^{*\top}Y^* = r_{XX}^{-1}r_{YX}.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;On the other hand, one can express the OLS estimate of $\beta$ in $(1)$ in terms of the OLS estimate of $\beta^*$ in $(2)$ as follows (this is easy to verify by substituting transformation definitions $(3.1)$ &amp;ndash; $(3.5)$ into $(2)$ then compare it with $(1)$, see also the aforementioned reference for derivation details):
$$
\begin{align*}
&amp;amp; \hat{\gamma} = s_Y\Lambda^{-1}\hat{\beta^*} = s_Y\Lambda^{-1}r_{XX}^{-1}r_{YX}, \tag{5.1} \\
&amp;amp; \hat{\beta}_0 = \bar{Y} - \begin{bmatrix}\bar{x}_1 &amp;amp; \cdots &amp;amp; \bar{x}_p \end{bmatrix}\hat{\gamma} = n^{-1}e^\top Y - n^{-1}e^\top \tilde{X}\hat{\gamma}. \tag{5.2}
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;It then follows by $(5.1), (5.2), (3.1), (3.2)$ that
$$
\begin{align*}
Y - \hat{Y} = Y - e\hat{\beta_0} - \tilde{X}\hat{\gamma}
= (I - n^{-1}ee^\top)Y - (I - n^{-1}ee^\top)\tilde{X}\hat{\gamma}
= s_Y\sqrt{n - 1}Y^* - s_Y\sqrt{n - 1}X^*r_{XX}^{-1}r_{YX},
\end{align*}
$$
whence by $(4)$ we conclude
$$
\begin{align*}
&amp;amp; (Y - \hat{Y})^\top(Y - \hat{Y}) \\
=&amp;amp; (n - 1)s_Y^2 Y^{*\top}Y^* - 2(n - 1)s_Y^2Y^{*\top}X^{*}r_{XX}^{-1}r_{YX} + (n - 1)s_Y^2r_{YX}^\top r_{XX}^{-1}X^{*\top}X^*r_{XX}^{-1}r_{YX} \\
=&amp;amp; (n - 1)s_Y^2(Y^{*\top}Y^* - r_{YX}^\top r_{XX}^{-1}r_{YX}). \tag{6}
\end{align*}
$$
It then follows by $(6)$ and the definition of $R^2$ that
$$
\begin{align*}
&amp;amp; R^2 = 1 - \frac{(Y - \hat{Y})^\top(Y - \hat{Y})}{Y^\top (I - n^{-1}ee^\top)Y}
= 1 - \frac{(n - 1)s_Y^2(Y^{*\top}Y^* - r_{YX}^\top r_{XX}^{-1}r_{YX})}{(n - 1)s_Y^2 Y^{*\top}Y^*}
= r_{YX}^\top r_{XX}^{-1}r_{YX}.
\end{align*}
$$
In the last equality, we used the identity $Y^{*\top}Y^* = 1$ (which follows from $(3.1)$ and $(3.3)$). This completes the proof.&lt;/p&gt;
&lt;h2 id=&#34;more-details&#34;&gt;More Details&lt;/h2&gt;
&lt;p&gt;Below is a more detailed derivation of $(5.1)$ and $(5.2)$.&lt;/p&gt;
&lt;p&gt;By model $(1)$, we have
$$
\begin{align*}
\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\gamma}
\end{bmatrix} =
\begin{bmatrix}
n &amp;amp; e^\top\tilde{X} \\
\tilde{X}^\top e &amp;amp; \tilde{X}^\top\tilde{X}
\end{bmatrix}^{-1}
\begin{bmatrix}
e^\top Y \\
\tilde{X}^\top Y
\end{bmatrix}. \tag{A.1}
\end{align*}
$$
Denote the matrix $\tilde{X}^\top(I - n^{-1}ee^\top)\tilde{X}$ by $C$, which by $(3.2)$ is equal to $(n - 1)\Lambda X^{*\top}X^*\Lambda$. It follows by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion&#34;&gt;block matrix inversion formula&lt;/a&gt; that
$$
\begin{align*}
\begin{bmatrix}
n &amp;amp; e^\top\tilde{X} \\
\tilde{X}^\top e &amp;amp; \tilde{X}^\top\tilde{X}
\end{bmatrix}^{-1}
= \begin{bmatrix}
n^{-1} + n^{-2}e^\top\tilde{X}C^{-1}\tilde{X}^\top e &amp;amp; -n^{-1}e^\top\tilde{X}C^{-1} \\
-n^{-1}C^{-1}\tilde{X}^\top e &amp;amp; C^{-1}
\end{bmatrix}. \tag{A.2}
\end{align*}
$$
Substituting $(A.2)$ into $(A.1)$ and using the idempotency of the matrix $I - n^{-1}ee^\top$ (also plugging definitions $(3.1)$ and $(3.2)$) then give
$$
\begin{align*}
&amp;amp; \hat{\gamma} = -n^{-1}C^{-1}\tilde{X}^\top ee^\top Y + C^{-1}\tilde{X}^\top Y \\
=&amp;amp; C^{-1}\tilde{X}^\top(I - n^{-1}ee^\top)Y \\
=&amp;amp; C^{-1}((I - n^{-1}ee^\top)\tilde{X})^\top (I - n^{-1}ee^\top)Y \\
=&amp;amp; (n - 1)^{-1}\Lambda^{-1}(X^{*\top}X^*)^{-1}\Lambda^{-1}(\sqrt{n - 1}X^*\Lambda)^\top \sqrt{n - 1}s_YY^* \\
=&amp;amp; s_Y\Lambda^{-1}(X^{*\top}X^*)^{-1}X^{*\top}Y^* \\
=&amp;amp; s_Y\Lambda^{-1}\hat{\beta^*}. \\[1em]
&amp;amp; \hat{\beta}_0 = n^{-1}e^\top Y + n^{-2}e^\top \tilde{X}C^{-1}\tilde{X}^\top ee^\top Y - n^{-1}e^\top\tilde{X}C^{-1}\tilde{X}^\top Y \\
=&amp;amp; n^{-1}e^\top Y - n^{-1}e^\top\tilde{X}(-n^{-1}C^{-1}\tilde{X}^\top ee^\top Y + C^{-1}\tilde{X}^\top Y) \\
=&amp;amp; n^{-1}e^\top Y - n^{-1}e^\top\tilde{X}\hat{\gamma},
\end{align*}
$$
which are exactly $(5.1)$ and $(5.2)$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Completeness of a Normal Family</title>
      <link>http://localhost:1313/ZX_BLOG/posts/2023-04-06-completeness/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ZX_BLOG/posts/2023-04-06-completeness/</guid>
      <description>&lt;p&gt;The problem is: How to show that $\{N(\theta,1):\theta \in \Omega\}$ is not a complete family of distributions when $\Omega$ is finite?&lt;/p&gt;
&lt;p&gt;For notational convenience, let $\varphi_{\theta}(x)$ denote the density of a $N(\theta, 1)$ random variable.&lt;/p&gt;
&lt;p&gt;One way of viewing this problem is that the condition
$$
\begin{align*}
\int_{-\infty}^\infty g(x)\varphi_\theta(x)dx = 0, \quad \theta \in \{-1, 0, 1\} \tag{1}
\end{align*}
$$
sets up a system of three equations. If we pick up $g(x) = ax^3 + bx^2 + cx + d$ from the family of cubic polynomials, then $(1)$ becomes a homogeneous system of three linear equations with four unknowns $a, b, c, d$.  By linear algebra theory, the dimensionality of the solution space to such a linear system is at least $1$ (because the rank of the associated coefficient matrix is at most $3$), implying that there are countless non-zero $g(x)$ satisfying $(1)$.&lt;/p&gt;
&lt;p&gt;Indeed, substituting $g(x) = ax^3 + bx^2 + cx + d$ and central moments of $\varphi_\theta(x)$ into $(1)$ yields
$$
\begin{align*}
b + d &amp;amp;= 0, \\
4a + 2b + c + d &amp;amp;= 0, \tag{2} \\
-4a + 2b - c + d &amp;amp;= 0.
\end{align*}
$$
One (of infinitely many) non-zero solution to $(2)$ is $a = -1, b = 0, c = 4, d = 0$, resulting $g(x) = -x^3 + 4x \neq 0$ (or more formally, $P_\theta(g(X) = 0) = P_\theta(X \in \{0, -2, 2\}) = 0 \neq 1$).  This shows that the family $\{N(\theta, 1): \theta \in \Omega\}$ is not complete.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;While the above construction works well for any parameter space with finite cardinality, it does not generalize to the case where the parameter space contains infinite members (e.g., the original linked exercise whose $\Omega = \{1, 2, 3, \ldots\}$). To deal with the latter case, note that, by the translation property of $\varphi_\theta$, the condition $E_\theta[g(X)] = 0$ for all $\theta \in \Omega$ is equivalent to&lt;br&gt;
$$
\begin{align*}
E[g(X + \theta)] = 0 \quad \text{ for all } \theta \in \Omega, \tag{3}
\end{align*}
$$
where &amp;ldquo;$E$&amp;rdquo; is with respect to the standard normal density $\varphi(x)$.  This observation implies that, when $\Omega = \{1, 2, \ldots\}$, $(3)$ automatically holds for $g$ such that $g(x + \theta) = g(x)$ for all $\theta = 1, 2, \ldots$ &lt;strong&gt;and&lt;/strong&gt; $E[g(X)] = 0$. Clearly, one candidate of such $g$ is any periodic odd function with periodicity $1$.  For example, $g(x) = \sin(\pi x)$ proposed in @jld&amp;rsquo;s answer.  As another (non-trigonometric) example, $g(x)$ can be taken to be
$$
\begin{align*}
g(x) = \frac{1}{2} - \left|x - \frac{1}{2}\right|, \quad 0 \leq x \leq 1.
\end{align*}
$$
And then copy this to intervals $[n, n + 1]$, $n = 1, 2, \ldots$ to complete $g$&amp;rsquo;s definition on $[0, +\infty)$.  Finally, define $g(x) = -g(-x)$ when $x &amp;lt; 0$.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
