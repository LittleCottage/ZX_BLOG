<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear Models on Zhanxiong&#39;s Blog</title>
    <link>http://localhost:1313/ZX_BLOG/tags/linear-models/</link>
    <description>Recent content in Linear Models on Zhanxiong&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 14 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/ZX_BLOG/tags/linear-models/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>An Alternative Representation of $R^2$</title>
      <link>http://localhost:1313/ZX_BLOG/posts/2023-11-14-r-squared/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ZX_BLOG/posts/2023-11-14-r-squared/</guid>
      <description>&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;This question was originally posted on &lt;a href=&#34;https://stats.stackexchange.com/questions/631291/are-these-two-definitions-of-the-coefficient-of-determination-r2-equal&#34;&gt;CrossValidated&lt;/a&gt;.  In more compact notations, it asks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Given a linear model
$$
\begin{align*}
Y := \begin{bmatrix} y_1 \ y_2 \ \vdots \ y_n \end{bmatrix}
= \begin{bmatrix} e &amp;amp; x_1 &amp;amp; x_2 &amp;amp; \cdots &amp;amp; x_p \end{bmatrix}\beta + \varepsilon,
\end{align*}
$$
show that the well-known &lt;a href=&#34;https://en.wikipedia.org/wiki/Coefficient_of_determination&#34;&gt;coefficient of determination&lt;/a&gt; has the following representation:
$$
\begin{align*}
R^2 = r_{YX}^\top r_{XX}^{-1}r_{YX},
\end{align*}
$$
where
$$
\begin{align*}
r_{YX} := \begin{bmatrix} r_{Y, x_1} \ \vdots \ r_{Y, x_p} \end{bmatrix}, \quad
r_{XX} := \begin{bmatrix} r_{x_i, x_j} \end{bmatrix}.
\end{align*}
$$
In other words, $R^2$ is a function of pair-wise Pearson correlations, which is surprisingly elegant.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;the-proof&#34;&gt;The Proof&lt;/h2&gt;
&lt;p&gt;A good question.  I have to confess that I haven&amp;rsquo;t seen the second expression of $R^2$ before (on the other hand, the &lt;a href=&#34;https://math.stackexchange.com/questions/129909/correlation-coefficient-and-determination-coefficient&#34;&gt;better-known result&lt;/a&gt; that $R^2$ is the squared Pearson correlation between the response and the predictor in simple linear regression is clearly a corollary of this statement), but here goes the (long) proof:&lt;/p&gt;
&lt;p&gt;To clarify, I am assuming you have $p$ regressors (excluding the intercept term), hence in matrix form, the model of your interest (by convention, a linear model contains an intercept $e$ of all ones) is:
$$
\begin{align*}
Y := \begin{bmatrix} y_1 \ y_2 \ \vdots \ y_n \end{bmatrix}
= \begin{bmatrix} e &amp;amp; x_1 &amp;amp; x_2 &amp;amp; \cdots &amp;amp; x_p \end{bmatrix}\beta + \varepsilon := \begin{bmatrix} e &amp;amp; \tilde{X}\end{bmatrix}\begin{bmatrix} \beta_0 \ \gamma \end{bmatrix} + \varepsilon, \tag{1}\label{1}
\end{align*}
$$
where $x_j = \begin{bmatrix} x_{1j} &amp;amp; \cdots &amp;amp; x_{nj}\end{bmatrix}^\top$, $j = 1, \ldots, p$, $\beta_0 \in \mathbb{R}$ is the coefficient corresponding to $e$, $\gamma \in \mathbb{R}^p$  is the coefficient corresponding to $\tilde{X} = \begin{bmatrix} x_1 &amp;amp; \cdots &amp;amp; x_p\end{bmatrix} \in \mathbb{R}^{n \times p}$.&lt;/p&gt;
&lt;p&gt;The form of your proposed $R_2^2$ inspires me considering the following form of the regression model (known as &lt;em&gt;Standardized Multiple Regression Model&lt;/em&gt;, see &lt;em&gt;Applied Linear Statistical Models&lt;/em&gt; by Kutner et al., Section 7.5) &amp;ndash; this is how I managed to bring all the Pearson correlations to the party:
\begin{align*}
Y^* = X^&lt;em&gt;\beta^&lt;/em&gt; + \varepsilon^&lt;em&gt;, \tag{2}\label{2}
\end{align&lt;/em&gt;}
where
\begin{align*}
&amp;amp; Y^* = \begin{bmatrix} \frac{y_1 - \bar{y}}{\sqrt{n - 1}s_Y} \ \vdots \ \frac{y_n - \bar{y}}{\sqrt{n - 1}s_Y} \end{bmatrix} =
(\sqrt{n - 1}s_Y)^{-1}(I - n^{-1}ee^\top)Y \in \mathbb{R}^{n \times 1}, \tag{3.1}\label{3.1} \
&amp;amp; X^* = \begin{bmatrix} \frac{x_1 - \bar{x}&lt;em&gt;1e}{\sqrt{n - 1}s_1} &amp;amp; \cdots &amp;amp;
\frac{x_p - \bar{x}&lt;em&gt;pe}{\sqrt{n - 1}s_p} \end{bmatrix} = (I - n^{-1}ee^\top)\tilde{X}(\sqrt{n - 1}\Lambda)^{-1} \in \mathbb{R}^{n \times p}, \tag{3.2}\label{3.2} \
&amp;amp; \bar{y} = \frac{1}{n}\sum&lt;/em&gt;{i = 1}^ny_i, ; s_Y^2 = \frac{1}{n - 1}\sum&lt;/em&gt;{i = 1}^n(y_i - \bar{y})^2, \tag{3.3}\label{3.3} \
&amp;amp; \bar{x}&lt;em&gt;j = n^{-1}\sum&lt;/em&gt;{i = 1}^n x_{ij}, ; s_j^2 = \frac{1}{n - 1}\sum_{i = 1}^n(x_{ij} - \bar{x}_j)^2, ; j = 1, \ldots, p, \tag{3.4}\label{3.4} \
&amp;amp; \Lambda = \operatorname{diag}(s_1, \ldots, s_p) \in \mathbb{R}^{p \times p}. \tag{3.5}\label{3.5}
\end{align*}
In words, $Y^&lt;em&gt;$ and $X^&lt;/em&gt;$ are simply standardized versions of original inputs $Y$ and $\tilde{X}$.&lt;/p&gt;
&lt;p&gt;With the above notations, on one hand, it is easy to verify that
\begin{align*}
r_{YX} := \begin{bmatrix} r_{Y, x_1} \ \vdots \ r_{Y, x_p} \end{bmatrix} = X^{&lt;em&gt;\top}Y^&lt;/em&gt;, \quad
r_{XX} := \begin{bmatrix} r_{x_i, x_j} \end{bmatrix} = X^{&lt;em&gt;\top}X^&lt;/em&gt;, \tag{4}\label{4}
\end{align*}
hence
\begin{align*}
\hat{\beta^&lt;em&gt;} = (X^{&lt;/em&gt;\top}X^&lt;em&gt;)^{-1}X^{&lt;/em&gt;\top}Y^* = r_{XX}^{-1}r_{YX}.
\end{align*}&lt;/p&gt;
&lt;p&gt;On the other hand, one can express the OLS estimate of $\beta$ in $\eqref{1}$ in terms of the OLS estimate of $\beta^&lt;em&gt;$ in $\eqref{2}$ as follows (this is easy to verify by substituting transformation definitions $\eqref{3.1}$ &amp;ndash; $\eqref{3.5}$ into $\eqref{2}$ then compare it with $\eqref{1}$, see also the aforementioned reference for derivation details):
\begin{align&lt;/em&gt;}
&amp;amp; \hat{\gamma} = s_Y\Lambda^{-1}\hat{\beta^&lt;em&gt;} = s_Y\Lambda^{-1}r_{XX}^{-1}r_{YX}, \tag{5.1}\label{5.1} \
&amp;amp; \hat{\beta}_0 = \bar{Y} - \begin{bmatrix}\bar{x}_1 &amp;amp; \cdots &amp;amp; \bar{x}_p \end{bmatrix}\hat{\gamma} = n^{-1}e^\top Y - n^{-1}e^\top \tilde{X}\hat{\gamma}. \tag{5.2}\label{5.2}
\end{align&lt;/em&gt;}&lt;/p&gt;
&lt;p&gt;It then follows by $\eqref{5.1}, \eqref{5.2}, \eqref{3.1}, \eqref{3.2}$ that
\begin{align*}
&amp;amp; Y - \hat{Y} = Y - e\hat{\beta}&lt;em&gt;0 - \tilde{X}\hat{\gamma} \
=&amp;amp; (I - n^{-1}ee^\top)Y - (I - n^{-1}ee^\top)\tilde{X}\hat{\gamma} \
=&amp;amp; s_Y\sqrt{n - 1}Y^* - s_Y\sqrt{n - 1}X^*r&lt;/em&gt;{XX}^{-1}r_{YX},
\end{align*}
whence by $\eqref{4}$ we conclude
\begin{align*}
&amp;amp; (Y - \hat{Y})^\top(Y - \hat{Y}) \
=&amp;amp; (n - 1)s_Y^2 Y^{&lt;em&gt;\top}Y^&lt;/em&gt; - 2(n - 1)s_Y^2Y^{&lt;em&gt;\top}X^{&lt;/em&gt;}r_{XX}^{-1}r_{YX} + (n - 1)s_Y^2r_{YX}^\top r_{XX}^{-1}X^{&lt;em&gt;\top}X^&lt;em&gt;r_{XX}^{-1}r_{YX} \
=&amp;amp; (n - 1)s_Y^2(Y^{&lt;/em&gt;\top}Y^&lt;/em&gt; - r_{YX}^\top r_{XX}^{-1}r_{YX}). \tag{6}\label{6}
\end{align*}
It then follows by $\eqref{6}$ and the definition of $R^2$ that
\begin{align*}
&amp;amp; R^2 = 1 - \frac{(Y - \hat{Y})^\top(Y - \hat{Y})}{Y^\top (I - n^{-1}ee^\top)Y} \
=&amp;amp; 1 - \frac{(n - 1)s_Y^2(Y^{&lt;em&gt;\top}Y^&lt;/em&gt; - r_{YX}^\top r_{XX}^{-1}r_{YX})}{(n - 1)s_Y^2 Y^{&lt;em&gt;\top}Y^&lt;/em&gt;} \
=&amp;amp; r_{YX}^\top r_{XX}^{-1}r_{YX}.
\end{align*}
In the last equality, we used the identity $Y^{&lt;em&gt;\top}Y^&lt;/em&gt; = 1$ (which follows from $\eqref{3.1}$ and $\eqref{3.3}$). This completes the proof.&lt;/p&gt;
&lt;h2 id=&#34;more-details&#34;&gt;More Details&lt;/h2&gt;
&lt;p&gt;Per OP&amp;rsquo;s request, below is a more detailed derivation of $\eqref{5.1}$ and $\eqref{5.2}$.&lt;/p&gt;
&lt;p&gt;By model $\eqref{1}$, we have
\begin{align*}
\begin{bmatrix}
\hat{\beta}_0 \
\hat{\gamma}
\end{bmatrix} =
\begin{bmatrix}
n &amp;amp; e^\top\tilde{X} \
\tilde{X}^\top e &amp;amp; \tilde{X}^\top\tilde{X}
\end{bmatrix}^{-1}
\begin{bmatrix}
e^\top Y \
\tilde{X}^\top Y
\end{bmatrix}. \tag{A.1}\label{A.1}
\end{align*}
Denote the matrix $\tilde{X}^\top(I - n^{-1}ee^\top)\tilde{X}$ by $C$, which by $\eqref{3.2}$ is equal to $(n - 1)\Lambda X^{&lt;em&gt;\top}X^&lt;/em&gt;\Lambda$. It follows by the &lt;a href=&#34;https://en.wikipedia.org/wiki/Block_matrix#Block_matrix_inversion&#34;&gt;block matrix inversion formula&lt;/a&gt; that
\begin{align*}
\begin{bmatrix}
n &amp;amp; e^\top\tilde{X} \
\tilde{X}^\top e &amp;amp; \tilde{X}^\top\tilde{X}
\end{bmatrix}^{-1}
= \begin{bmatrix}
n^{-1} + n^{-2}e^\top\tilde{X}C^{-1}\tilde{X}^\top e &amp;amp; -n^{-1}e^\top\tilde{X}C^{-1} \
-n^{-1}C^{-1}\tilde{X}^\top e &amp;amp; C^{-1}
\end{bmatrix}. \tag{A.2}\label{A.2}
\end{align*}&lt;/p&gt;
&lt;p&gt;Substituting $\eqref{A.2}$ into $\eqref{A.1}$ and using the idempotency of the matrix $I - n^{-1}ee^\top$ (also plugging definitions $\eqref{3.1}$ and $\eqref{3.2}$) then give
\begin{align*}
&amp;amp; \hat{\gamma} = -n^{-1}C^{-1}\tilde{X}^\top ee^\top Y + C^{-1}\tilde{X}^\top Y \
=&amp;amp; C^{-1}\tilde{X}^\top(I - n^{-1}ee^\top)Y \
=&amp;amp; C^{-1}((I - n^{-1}ee^\top)\tilde{X})^\top (I - n^{-1}ee^\top)Y \
=&amp;amp; (n - 1)^{-1}\Lambda^{-1}(X^{&lt;em&gt;\top}X^&lt;/em&gt;)^{-1}\Lambda^{-1}(\sqrt{n - 1}X^&lt;em&gt;\Lambda)^\top \sqrt{n - 1}s_YY^&lt;/em&gt; \
=&amp;amp; s_Y\Lambda^{-1}(X^{&lt;em&gt;\top}X^&lt;/em&gt;)^{-1}X^{&lt;em&gt;\top}Y^&lt;/em&gt; \
=&amp;amp; s_Y\Lambda^{-1}\hat{\beta^&lt;em&gt;}. \[1em]
&amp;amp; \hat{\beta}_0 = n^{-1}e^\top Y + n^{-2}e^\top \tilde{X}C^{-1}\tilde{X}^\top ee^\top Y - n^{-1}e^\top\tilde{X}C^{-1}\tilde{X}^\top Y \
=&amp;amp; n^{-1}e^\top Y - n^{-1}e^\top\tilde{X}(-n^{-1}C^{-1}\tilde{X}^\top ee^\top Y + C^{-1}\tilde{X}^\top Y) \
=&amp;amp; n^{-1}e^\top Y - n^{-1}e^\top\tilde{X}\hat{\gamma},
\end{align&lt;/em&gt;}
which are exactly $\eqref{5.1}$ and $\eqref{5.2}$.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
