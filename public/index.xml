<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Zhanxiong&#39;s Blog</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Zhanxiong&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gaussian Kernel Properties</title>
      <link>http://localhost:1313/posts/2025-06-09-gaussian/</link>
      <pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/2025-06-09-gaussian/</guid>
      <description>&lt;p&gt;Your question can be reframed as: why the statement (a) $X_t \mid X_{t - 1} \sim N(\sqrt{1 - \beta_t}X_{t - 1}, \beta_t I)$ is equivalent to the statement (b) $X_t$ has the representation $X_t = \sqrt{1 - \beta_t}X_{t - 1} + \sqrt{\beta_t}Z_t$, where $Z_t \sim N(0, I)$ is independent of $X_{t - 1}$?&lt;/p&gt;
&lt;p&gt;While Thomas Lumley has given you a heuristic answer, I agree that you are absolutely entitled to request a more rigorous argument.&lt;/p&gt;
&lt;p&gt;The direction $(b) \Rightarrow (a)$ is an immediate application of the theorem below (Lemma 8.7 in Foundations of Modern Probability (3rd edition) by Olav Kallenberg):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For any random elements $\xi \perp \eta$ in $S, T$ and a measurable function $f: S \times T \to U$, where $S, T, U$ are Borel, define $X = f(\xi, \eta)$. Then $\mathcal{L}(X \mid \xi) = \mathcal{L}\{f(x, \eta)\}|_{x = \xi}$ a.s.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Here &amp;ldquo;$\mathcal{L}(X)$ / $\mathcal{L}(X \mid \xi)$&amp;rdquo; denotes the law (i.e., distribution) of $X$ / conditional law of $X$ given $\xi$. To apply this result, simply let $\xi = X_{t - 1}, \eta = Z_t$, and $X = X_t = \sqrt{1 - \beta_t}X_{t - 1} + \sqrt{\beta_t}Z_t$. If you are interested in the proof to this lemma, check the reference by yourself.&lt;/p&gt;
&lt;p&gt;For the direction $(a) \Rightarrow (b)$, it suffices to prove the conditional distribution of $Z_t := \beta_t^{-1/2}(X_t - \sqrt{1 - \beta_t}X_{t - 1})$ given $X_{t - 1}$ is $N(0, I)$, hence it is independent of $X_{t - 1}$. To this end, we can apply Theorem 8.5(ii) in the same reference (which concerns with the disintegration of the joint distribution of $(X_{t - 1}, X_t)$).  By this theorem, for a Borel set $H \in \mathscr{R}^n$, we have (where the notation $aH + b$ denotes the set ${ax + b: x \in H}$):
$$
\begin{align*}
&amp;amp;\ P(Z_t \in H \mid X_{t - 1}) \\
=&amp;amp;\ P\left(X_t \in \sqrt{\beta_t}H + \sqrt{1 - \beta_t}X_{t - 1} \mid X_{t - 1}\right) \\
=&amp;amp;\ \int_{\sqrt{\beta_t}H + \sqrt{1 - \beta_t}X_{t - 1}}
(2\pi\beta_t)^{-n/2}\exp\left(-\frac{\beta_t^{-n}}{2}(x - \sqrt{1 - \beta_t}X_{t - 1})^\top(x - \sqrt{1 - \beta_t}X_{t - 1})\right)dx \\
=&amp;amp;\ \int_H (2\pi)^{-n/2}\exp\left(-\frac{1}{2}z^\top z\right)dz.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;where the last step uses the transformation $z = \beta_t^{-n/2}(x - \sqrt{1 - \beta_t}X_{t - 1})$ and the property of the Lebesgue integration. The last expression shows that the conditional distribution of $Z_t$ given $X_{t - 1}$ is exactly $N(0, I)$.  This completes the proof.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Completeness of a Normal Family</title>
      <link>http://localhost:1313/posts/2023-04-06-completeness/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/posts/2023-04-06-completeness/</guid>
      <description>&lt;p&gt;The problem is: How to show that $\{N(\theta,1):\theta \in \Omega\}$ is not a complete family of distributions when $\Omega$ is finite?&lt;/p&gt;
&lt;p&gt;For notational convenience, let $\varphi_{\theta}(x)$ denote the density of a $N(\theta, 1)$ random variable.&lt;/p&gt;
&lt;p&gt;One way of viewing this problem is that the condition
$$
\begin{align*}
\int_{-\infty}^\infty g(x)\varphi_\theta(x)dx = 0, \quad \theta \in \{-1, 0, 1\} \tag{1}
\end{align*}
$$
sets up a system of three equations. If we pick up $g(x) = ax^3 + bx^2 + cx + d$ from the family of cubic polynomials, then $(1)$ becomes a homogeneous system of three linear equations with four unknowns $a, b, c, d$.  By linear algebra theory, the dimensionality of the solution space to such a linear system is at least $1$ (because the rank of the associated coefficient matrix is at most $3$), implying that there are countless non-zero $g(x)$ satisfying $(1)$.&lt;/p&gt;
&lt;p&gt;Indeed, substituting $g(x) = ax^3 + bx^2 + cx + d$ and central moments of $\varphi_\theta(x)$ into $(1)$ yields
$$
\begin{align*}
b + d &amp;amp;= 0, \\
4a + 2b + c + d &amp;amp;= 0, \tag{2} \\
-4a + 2b - c + d &amp;amp;= 0.
\end{align*}
$$
One (of infinitely many) non-zero solution to $(2)$ is $a = -1, b = 0, c = 4, d = 0$, resulting $g(x) = -x^3 + 4x \neq 0$ (or more formally, $P_\theta(g(X) = 0) = P_\theta(X \in \{0, -2, 2\}) = 0 \neq 1$).  This shows that the family $\{N(\theta, 1): \theta \in \Omega\}$ is not complete.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;While the above construction works well for any parameter space with finite cardinality, it does not generalize to the case where the parameter space contains infinite members (e.g., the original linked exercise whose $\Omega = \{1, 2, 3, \ldots\}$). To deal with the latter case, note that, by the translation property of $\varphi_\theta$, the condition $E_\theta[g(X)] = 0$ for all $\theta \in \Omega$ is equivalent to&lt;br&gt;
$$
\begin{align*}
E[g(X + \theta)] = 0 \quad \text{ for all } \theta \in \Omega, \tag{3}
\end{align*}
$$
where &amp;ldquo;$E$&amp;rdquo; is with respect to the standard normal density $\varphi(x)$.  This observation implies that, when $\Omega = \{1, 2, \ldots\}$, $(3)$ automatically holds for $g$ such that $g(x + \theta) = g(x)$ for all $\theta = 1, 2, \ldots$ &lt;strong&gt;and&lt;/strong&gt; $E[g(X)] = 0$. Clearly, one candidate of such $g$ is any periodic odd function with periodicity $1$.  For example, $g(x) = \sin(\pi x)$ proposed in @jld&amp;rsquo;s answer.  As another (non-trigonometric) example, $g(x)$ can be taken to be
$$
\begin{align*}
g(x) = \frac{1}{2} - \left|x - \frac{1}{2}\right|, \quad 0 \leq x \leq 1.
\end{align*}
$$
And then copy this to intervals $[n, n + 1]$, $n = 1, 2, \ldots$ to complete $g$&amp;rsquo;s definition on $[0, +\infty)$.  Finally, define $g(x) = -g(-x)$ when $x &amp;lt; 0$.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
